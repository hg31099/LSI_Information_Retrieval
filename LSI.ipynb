{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing the documents in corpus (Collecting all terms) . . . Done\n",
      "Query is:\n",
      "[0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "Original :\n",
      "[[0, 0, 1], [1, 0, 0], [1, 0, 1], [0, 1, 1], [1, 0, 0], [1, 1, 1], [1, 0, 0], [1, 0, 0], [0, 1, 1], [1, 1, 1], [0, 1, 0], [0, 1, 0], [1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1]]\n",
      "\n",
      "FactorS of the given term_incidence_matrix by Singular Value Decomposition are:\n",
      "\n",
      "U Matrix= [[-0.1324  -0.19821 -0.35355]\n",
      " [-0.15704  0.33422 -0.     ]\n",
      " [-0.28944  0.136   -0.35355]\n",
      " [-0.2648  -0.39643 -0.     ]\n",
      " [-0.15704  0.33422 -0.     ]\n",
      " [-0.42184 -0.06221  0.     ]\n",
      " [-0.15704  0.33422 -0.     ]\n",
      " [-0.15704  0.33422 -0.     ]\n",
      " [-0.2648  -0.39643 -0.     ]\n",
      " [-0.42184 -0.06221  0.     ]\n",
      " [-0.1324  -0.19821  0.35355]\n",
      " [-0.1324  -0.19821  0.35355]\n",
      " [-0.28944  0.136    0.35355]\n",
      " [-0.28944  0.136    0.35355]\n",
      " [-0.28944  0.136   -0.35355]\n",
      " [-0.1324  -0.19821 -0.35355]] \n",
      "\n",
      "S Matrix= [[4.09201 0.      0.     ]\n",
      " [0.      2.29247 0.     ]\n",
      " [0.      0.      2.     ]] \n",
      "\n",
      "V_transpose_matrix= [[-0.64262 -0.54177 -0.54177]\n",
      " [ 0.76618 -0.4544  -0.4544 ]\n",
      " [-0.       0.70711 -0.70711]] \n",
      "\n",
      "V_matrix= [[-0.64262  0.76618 -0.     ]\n",
      " [-0.54177 -0.4544   0.70711]\n",
      " [-0.54177 -0.4544  -0.70711]]\n",
      "\n",
      "q= [0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0] \n",
      "\n",
      "qt= [0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Uk= [[-0.1324  -0.19821]\n",
      " [-0.15704  0.33422]\n",
      " [-0.28944  0.136  ]\n",
      " [-0.2648  -0.39643]\n",
      " [-0.15704  0.33422]\n",
      " [-0.42184 -0.06221]\n",
      " [-0.15704  0.33422]\n",
      " [-0.15704  0.33422]\n",
      " [-0.2648  -0.39643]\n",
      " [-0.42184 -0.06221]\n",
      " [-0.1324  -0.19821]\n",
      " [-0.1324  -0.19821]\n",
      " [-0.28944  0.136  ]\n",
      " [-0.28944  0.136  ]\n",
      " [-0.28944  0.136  ]\n",
      " [-0.1324  -0.19821]] \n",
      "\n",
      "Vk= [[-0.64262  0.76618]\n",
      " [-0.54177 -0.4544 ]\n",
      " [-0.54177 -0.4544 ]] \n",
      "\n",
      "sk= [[4.09201 0.     ]\n",
      " [0.      2.29247]]\n",
      "\n",
      "qnew= [-0.20618 -0.05427]\n",
      "[0.4264  0.90453 0.90453]\n",
      "document ranks are:\n",
      "[1 2 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import re\n",
    "\n",
    "# corpus=3\n",
    "# term_incidence_matrix = np.array([[2,0,1],[0,1,2],[1,1,0],[0,1,0],[1,0,0],[1,1,1],[0,1,1],[1,2,1],[2,0,2],[1,2,0],[0,3,1]],dtype=np.float32)   \n",
    "# query=np.array([[1],[0],[0],[0],[0],[1],[0],[0],[0],[1],[0]],dtype=np.float32)\n",
    "\n",
    "no_files_in_corpus = 3\n",
    "term_incidence_matrix,term_collection=build_corpus(no_files_in_corpus)\n",
    "\n",
    "query = \"brown fox jump\"\n",
    "token = tokenize(query)\n",
    "\n",
    "query = [0 for i in range(len(term_collection))]\n",
    "for t in token:\n",
    "    index = term_collection.get(t, None)\n",
    "\n",
    "    if index is not None:\n",
    "        query[index] = query[index] + 1\n",
    "\n",
    "'''\n",
    "Rebuild the query\n",
    "'''\n",
    "query = np.array(query)\n",
    "print(\"Query is:\")\n",
    "print(query)\n",
    "\n",
    "print(\"Original :\") \n",
    "print(term_incidence_matrix) \n",
    "\n",
    "# Compute the factor by Singular Value \n",
    "# Decomposition \n",
    "np.set_printoptions(precision=5,suppress=True)\n",
    "U, S, Vt = np.linalg.svd(term_incidence_matrix, full_matrices=False) \n",
    "S=np.diag(S)\n",
    "V=np.transpose(Vt)\n",
    "\n",
    "# Print the result \n",
    "print(\"\\nFactorS of the given term_incidence_matrix by Singular Value Decomposition are:\") \n",
    "print(\"\\nU Matrix=\", U, \"\\n\\nS Matrix=\", S, \"\\n\\nV_transpose_matrix=\", Vt, \"\\n\\nV_matrix=\", V) \n",
    "\n",
    "qt=np.transpose(query)\n",
    "print(\"\\nq=\", query, \"\\n\\nqt=\", qt) \n",
    "\n",
    "k=2\n",
    "Uk=U[:,:k]\n",
    "Vk=V[:,:k]\n",
    "Sk=S[:k,:k]\n",
    "Ski=np.linalg.inv(Sk) \n",
    "\n",
    "\n",
    "doc=[[] for i in range(no_files_in_corpus)]\n",
    "for i in range(0,no_files_in_corpus):\n",
    "    doc[i]=Vk[i]\n",
    "\n",
    "print(\"\\nUk=\", Uk, \"\\n\\nVk=\", Vk, \"\\n\\nsk=\", Sk) \n",
    "qnew=np.dot((np.dot(qt,Uk)),Ski)\n",
    "print(\"\\nqnew=\", qnew)\n",
    "\n",
    "\n",
    "magqnew=np.linalg.norm(qnew)\n",
    "sim=np.zeros(no_files_in_corpus)\n",
    "\n",
    "for i in range(0,no_files_in_corpus):\n",
    "    sim[i]=(np.dot(qnew,doc[i])/(magqnew*np.linalg.norm(doc[i])))\n",
    "\n",
    "print(sim)\n",
    "doc_index = np.argsort(sim)[::-1][:no_files_in_corpus]\n",
    "print(\"document ranks are:\")\n",
    "print(doc_index) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, term_collection = None):\n",
    "    '''\n",
    "    Splitting the text aginst some pre-defined delimiter\n",
    "    '''\n",
    "    '''Tokenization'''\n",
    "    split_chars = [\" \", \"\\n\", \"\\-\", \",\", \"\\.\", \"\\\"\", \"\\(\", \"\\)\", \"\\?\", \"\\[\", \"\\]\", \"\\*\", \";\", \"\\\\\", \"/\"]\n",
    "    delimiter = '|'.join(split_chars)\n",
    "    tokens = re.split(delimiter, text)\n",
    "\n",
    "    tokens = list(map(lambda x: x.lower(), tokens))\n",
    "\n",
    "    if term_collection is not None:\n",
    "        for t in tokens:\n",
    "            term_collection.append(t)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "    '''END'''\n",
    "\n",
    "\n",
    "def build_corpus(no_files_in_corpus):\n",
    "    '''\n",
    "    Building corpus as term_collection\n",
    "    '''\n",
    "    term_collection, all_tokens = [], []\n",
    "\n",
    "    '''\n",
    "    Collecting all terms present in the collection\n",
    "    '''\n",
    "    print(\"\\nProcessing the documents in corpus (Collecting all terms) . . . \", end = '')\n",
    "    for i in range(1,no_files_in_corpus+1):\n",
    "        with open('doc' + str(i) + '.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "            '''Storing document-wise and in-total at one time'''\n",
    "            all_tokens.append(tokenize(text, term_collection))\n",
    "\n",
    "    # Sorting the terms lexicographically and make it a dictionary\n",
    "    term_collection = sorted(list(set(term_collection)))\n",
    "    term_collection = {term : index for index, term in enumerate(term_collection)}\n",
    "\n",
    "    '''\n",
    "    Building term-document incedence matrix\n",
    "    '''\n",
    "    corpus = [[0 for j in range(no_files_in_corpus)] for i in range(len(term_collection))]\n",
    "\n",
    "    for doc_index, doc_tokens in enumerate(all_tokens):\n",
    "        for token in doc_tokens:\n",
    "            row_index, col_index = term_collection[token], doc_index\n",
    "            corpus[row_index][col_index] = corpus[row_index][col_index] + 1\n",
    "\n",
    "    '''\n",
    "    END\n",
    "    '''\n",
    "    print(\"Done\")\n",
    "\n",
    "    return corpus, term_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
